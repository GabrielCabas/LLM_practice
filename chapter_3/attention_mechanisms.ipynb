{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a64ad35",
   "metadata": {},
   "source": [
    "# Coding attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668e314",
   "metadata": {},
   "source": [
    "We will implement 4 different attention mechanisms.\n",
    " - simplified self-attention\n",
    " - self-attention\n",
    " - causal attention\n",
    " - multi head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc05c7f9",
   "metadata": {},
   "source": [
    "## Simplified self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "429d3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "867c1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy input embeddings for 6 tokens, each with an embedding size of 3\n",
    "inputs = torch.tensor(\n",
    "    [[0.42, 0.15, 0.89],\n",
    "    [0.78, 0.33, 0.21],\n",
    "    [0.12, 0.44, 0.67],\n",
    "    [0.56, 0.91, 0.73],\n",
    "    [0.34, 0.29, 0.85],\n",
    "    [0.63, 0.11, 0.49]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86543b6c",
   "metadata": {},
   "source": [
    "Attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6ef5c445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5640, 0.7614, 0.3795, 0.8904, 0.5394, 0.6306])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = inputs[1] # Second word\n",
    "attention_scores_2 = torch.empty(inputs.shape[0]) # 6 scores\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attention_scores_2[i] = torch.dot(x_i, query) #Dot product\n",
    "attention_scores_2 #This vector says how similar each word is to the second word\n",
    "#Second item is the greatest in the list, because is respect to himself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7855708c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1543, 0.1880, 0.1283, 0.2139, 0.1506, 0.1649])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_2 = torch.softmax(attention_scores_2, dim=0) #Now sum to 1\n",
    "attention_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "df48f105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5017, 0.3981, 0.6277])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attention_weights_2[i] * x_i\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fc97b",
   "metadata": {},
   "source": [
    "This is the context vector for only the item 2 (query).\n",
    "\n",
    "Now, we would like to have all context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "837e284d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6])\n",
      "torch.Size([6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4657, 0.3874, 0.6732],\n",
       "        [0.5017, 0.3981, 0.6277],\n",
       "        [0.4606, 0.4091, 0.6722],\n",
       "        [0.4790, 0.4538, 0.6663],\n",
       "        [0.4641, 0.3983, 0.6740],\n",
       "        [0.4861, 0.3826, 0.6464]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = inputs @ inputs.T  # Shape (6, 6)\n",
    "print(attention_scores.shape)\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)  # Shape (6,\n",
    "print(attention_weights.shape)\n",
    "context_vectors = attention_weights @ inputs\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd57805f",
   "metadata": {},
   "source": [
    "So, the steps are:\n",
    " - **Attention Scores**: Raw similarity between words (dot product)\n",
    " - **Attention Weights**: Normalized scores that sum to 1 (softmax)  \n",
    " - **Context Vectors**: Weighted combination of all words (final representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052d839",
   "metadata": {},
   "source": [
    "## Attention with trainable parameters\n",
    "\n",
    "Now, let's compute attention weights with trainable parameters.\n",
    "We will introduce three matrices:\n",
    " - W_q: query\n",
    " - W_k: key\n",
    " - W_v: value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f26021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1] #Input of embedding size (3)\n",
    "d_out = 2 #Output of embedding size (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "74f819d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_k = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_v = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f023a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = x_2 @ W_q\n",
    "key_2 = x_2 @ W_k\n",
    "value_2 = x_2 @ W_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7a1c460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.6035, 0.8222]), tensor([0.7056, 0.5192]), tensor([0.5968, 0.9680]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2, key_2, value_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "21072402",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = inputs @ W_k\n",
    "values = inputs @ W_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328fcbb",
   "metadata": {},
   "source": [
    "Okay, let's get the attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0211f0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8740, 0.8528, 0.8435, 1.5476, 0.9330, 0.7289])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec13c09",
   "metadata": {},
   "source": [
    "An then the attention weights. We will normalize by sqrt(len(keys)). This tries to improve the training performance by avoiding small gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f4464dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1534, 0.1511, 0.1501, 0.2470, 0.1599, 0.1384])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2 / keys.shape[-1] ** 0.5, dim=-1)\n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9972d",
   "metadata": {},
   "source": [
    "Let's get the context vector for the second word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c1611b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8122, 1.0227])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbfef48",
   "metadata": {},
   "source": [
    "Query key and value are borrowed from the domain of information retrieval and databases, where similar concepts are used to store, search and retrieve information.\n",
    "\n",
    "Query is analogous to a search query ina database. It represents the current item.\n",
    "\n",
    "Key is like a database key used for indexing and searching. Each item in the input sequence has an associated key.\n",
    "\n",
    "Value is the value in a key-value pair in a database. Represents the actual content of the input items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be819df5",
   "metadata": {},
   "source": [
    "## Compact self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ccf63c",
   "metadata": {},
   "source": [
    "We will use a class to summary the concept of self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "794dcae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "        context_vector = attn_weights @ values\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b372e562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6221, 0.5223],\n",
      "        [0.6230, 0.5217],\n",
      "        [0.6220, 0.5222],\n",
      "        [0.6279, 0.5256],\n",
      "        [0.6225, 0.5226],\n",
      "        [0.6216, 0.5214]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sa_v1 = SelfAttentionV1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f65e8e4",
   "metadata": {},
   "source": [
    "Let's do the same but using linear layers. These use more optimized initial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "154e4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out)\n",
    "        self.W_key = nn.Linear(d_in, d_out)\n",
    "        self.W_value = nn.Linear(d_in, d_out)\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** 0.5), dim = -1)\n",
    "        context_vector = attn_weights @ values\n",
    "        return context_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e5ffa2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4356, -0.6395],\n",
      "        [-0.4389, -0.6404],\n",
      "        [-0.4352, -0.6389],\n",
      "        [-0.4367, -0.6394],\n",
      "        [-0.4354, -0.6393],\n",
      "        [-0.4375, -0.6401]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869b90c",
   "metadata": {},
   "source": [
    "## Hiding future words with causal attention\n",
    "\n",
    "For many LLM tasks (as GPT), you will want the self-attention mechanism to consider only the tokens that appear prior the current position.\n",
    "\n",
    "This is causal/masked attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "899b4905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5268, 0.3769, 0.4977, 0.8049, 0.5535, 0.3767],\n",
       "        [0.2446, 0.1040, 0.2256, 0.3075, 0.2525, 0.1425],\n",
       "        [0.5180, 0.3810, 0.4902, 0.8012, 0.5450, 0.3752],\n",
       "        [0.4012, 0.2659, 0.3774, 0.5933, 0.4203, 0.2772],\n",
       "        [0.5255, 0.3799, 0.4968, 0.8066, 0.5525, 0.3776],\n",
       "        [0.3706, 0.2235, 0.3469, 0.5275, 0.3868, 0.2460]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "key = sa_v2.W_key(inputs)\n",
    "value = sa_v2.W_value(inputs)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "attn_scores #Squared matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3537c",
   "metadata": {},
   "source": [
    "Let's replace the upper triangle with -inf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0501c8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5268,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.2446, 0.1040,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.5180, 0.3810, 0.4902,   -inf,   -inf,   -inf],\n",
       "        [0.4012, 0.2659, 0.3774, 0.5933,   -inf,   -inf],\n",
       "        [0.5255, 0.3799, 0.4968, 0.8066, 0.5525,   -inf],\n",
       "        [0.3706, 0.2235, 0.3469, 0.5275, 0.3868, 0.2460]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal = 1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f66d2",
   "metadata": {},
   "source": [
    "Now, if we use softmax, rows will sum 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9a6a1f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5248, 0.4752, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3462, 0.3143, 0.3395, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2477, 0.2251, 0.2435, 0.2837, 0.0000, 0.0000],\n",
       "        [0.1953, 0.1762, 0.1913, 0.2382, 0.1990, 0.0000],\n",
       "        [0.1687, 0.1520, 0.1659, 0.1884, 0.1706, 0.1544]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim = 1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "24cdad8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9036, 0.9912],\n",
       "        [0.7578, 0.9802],\n",
       "        [0.7242, 0.8928],\n",
       "        [0.8062, 1.0259],\n",
       "        [0.8215, 1.0179],\n",
       "        [0.7979, 0.9951]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec = attn_weights @ values\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670bf39",
   "metadata": {},
   "source": [
    "Making additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2d35789c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.2857, 0.2641, 0.2108],\n",
       "        [0.2204, 0.2686, 0.0000, 0.0000, 0.2151, 0.2356],\n",
       "        [0.2491, 0.1785, 0.2356, 0.0000, 0.2555, 0.1920],\n",
       "        [0.0000, 0.1898, 0.2029, 0.0000, 0.0000, 0.1753],\n",
       "        [0.2666, 0.1781, 0.2172, 0.3042, 0.2611, 0.0000],\n",
       "        [0.2523, 0.2313, 0.1935, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import Dropout\n",
    "dropout = Dropout(0.3)\n",
    "dropout(attention_weights) #Drop some random cells (~30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "36cb1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    # Implements causal (masked) self-attention for a single attention head.\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        # Linear layers to project input to queries, keys, and values\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Register a causal mask to prevent attending to future tokens\n",
    "        # This is a buffer, which means it's part of the model but doesn't need gradients\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "            diagonal=1)\n",
    "        ) #It saves memory and is faster.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, num_tokens, d_in)\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        queries = self.W_query(x)  # (batch_size, num_tokens, d_out)\n",
    "        keys = self.W_key(x)       # (batch_size, num_tokens, d_out)\n",
    "        values = self.W_value(x)   # (batch_size, num_tokens, d_out)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = queries @ keys.transpose(1, 2)  # (batch_size, num_tokens, num_tokens)\n",
    "        # Apply causal mask (upper triangle set to -inf)\n",
    "        attn_scores.masked_fill(self.mask.bool()[:num_tokens, :num_tokens],\n",
    "            -torch.inf)\n",
    "        #It used the registered buffer to mask the future tokens.\n",
    "        # Softmax over the last dimension (tokens), then apply dropout\n",
    "        attn_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = 1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        context_vec = attn_weights @ values  # (batch_size, num_tokens, d_out)\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1e4cd8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 2])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "context_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b42a4",
   "metadata": {},
   "source": [
    "## Multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6643dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "            for _ in range(num_heads)\n",
    "        ]) #We create a list of causal attention heads.\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim = -1) #We concatenate the outputs of all heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e9a47ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 4])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads = 2)\n",
    "context_vecs = mha(batch)\n",
    "context_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa89da",
   "metadata": {},
   "source": [
    "## Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2375b",
   "metadata": {},
   "source": [
    "Let's implement a more efficient multi-head attention. We can improve the last implementation by processing the heads in parallel. One way to achieve this is by computing the outputs for all attention heads simultaneously via matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class implements a more efficient version of multi-head self-attention.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "        context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        # Linear layer to project input to queries\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # Linear layer to project input to keys\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # Linear layer to project input to values\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # Output projection after concatenating all heads\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Register a causal mask to prevent attending to future tokens\n",
    "        #Again, this is a buffer, which means it's part of the model but doesn't need gradients.\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "            diagonal=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # Project input to keys, queries, and values\n",
    "        #The output tensor shape is (batch, num_tokens, d_out)\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        # Reshape for multi-head: (batch, tokens, heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Move heads dimension forward: (batch, heads, tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores: (batch, heads, tokens, tokens)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        # Create boolean mask for causal attention\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        # Apply mask: set future positions to -inf\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # Softmax over last dimension to get attention weights\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        # Concatenate all heads\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "        # Final output projection\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c1ff1b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0988,  0.3279],\n",
       "         [-0.2344,  0.4464],\n",
       "         [-0.2199,  0.4497],\n",
       "         [-0.2460,  0.4768],\n",
       "         [-0.2205,  0.4535],\n",
       "         [-0.2223,  0.4507]],\n",
       "\n",
       "        [[-0.0988,  0.3279],\n",
       "         [-0.2344,  0.4464],\n",
       "         [-0.2199,  0.4497],\n",
       "         [-0.2460,  0.4768],\n",
       "         [-0.2205,  0.4535],\n",
       "         [-0.2223,  0.4507]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads = 2)\n",
    "context_vecs = mha(batch)\n",
    "display(context_vecs)\n",
    "print(context_vecs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
